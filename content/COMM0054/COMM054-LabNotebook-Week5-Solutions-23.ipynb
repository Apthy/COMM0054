{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a2bfe3",
   "metadata": {},
   "source": [
    "# Data Science Principles and Practices (COMM054) Lab Week 5\n",
    "\n",
    "Follow the instructions to complete each of these tasks. This set of exercises focusses on writing basic Python code to perform estimation using **SciPy**. Do not worry if you do not complete them all in the timetabled lab session.\n",
    "\n",
    "This is not assessed but will help you gain practical experience for the module exam and coursework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd84327",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "As in previous weeks, we will be using NumPy, SciPy, and Pyplot from Matplotlib, so import these first:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae32fba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:37:17.517275655Z",
     "start_time": "2023-10-26T14:37:17.089429479Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c1fec",
   "metadata": {},
   "source": [
    "# Estimation of population mean and population variance\n",
    "\n",
    "Recall the estimators for the population mean and population variance discussed in lectures:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\overline{x} &= \\frac{1}{n}\\sum_{i=1}^n x_i \\qquad\\qquad\\text{sample mean}\\\\\n",
    "s^2_\\text{biased} &= \\frac{1}{n}\\sum_{i=1}^n (x_i-\\overline{x})^2\\qquad\\text{biased sample variance}\\\\\n",
    "s^2_\\text{unbiased} &= \\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\overline{x})^2\\qquad\\text{unbiased sample variance}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $n$ is the sample size. \n",
    "\n",
    "Let us first evaluate these by drawing 6 samples from a Poisson distribution (which you will recall has equal mean and variance):\n",
    "\n",
    "```python\n",
    "mu, n = 5, 6\n",
    "x_samples = ss.poisson.rvs(mu, size=n)\n",
    "\n",
    "mean = np.mean(x_samples)\n",
    "var_b = np.var(x_samples)\n",
    "var_ub = np.var(x_samples, ddof=1)\n",
    "print(\"The true values of the mean and variance are\", mu, \"and\", mu, \"respectively\")\n",
    "print(\"The samples are\", x_samples)\n",
    "print(\"The sample mean is\", mean)\n",
    "print(\"The biased sample variance is\", var_b)\n",
    "print(\"The unbiased sample variance is\", var_ub)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cdcd91",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mu, n = 5, 6\n",
    "x_samples = ss.poisson.rvs(mu, size=n)\n",
    "\n",
    "mean = np.mean(x_samples)\n",
    "var_b = np.var(x_samples)\n",
    "var_ub = np.var(x_samples, ddof=1)\n",
    "print(\"The true values of the mean and variance are\", mu, \"and\", mu, \"respectively\")\n",
    "print(\"The samples are\", x_samples)\n",
    "print(\"The sample mean is\", mean)\n",
    "print(\"The biased sample variance is\", var_b)\n",
    "print(\"The unbiased sample variance is\", var_ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b3e1e",
   "metadata": {},
   "source": [
    "Unbiased estimators have the property that their expectation equals the true value of the parameter. This means that if we consider a large number of experiments (in each case, taking a fixed number of samples $n$, and calculating the estimator on these samples), then the average of the estimates will tend to the true value of the parameter as the number of experiments increases. \n",
    "\n",
    "Let's see this in action for our case: we consider running 10000 experiments, where in each case we sample 6 values from the Poisson distribution. We calculate the sample mean, biased sample variance, and unbiased sample variance in each case, and average over the number of experiments:\n",
    "\n",
    "```python\n",
    "mu, n = 5, 6\n",
    "num_experiments = 10000\n",
    "\n",
    "mean_average, var_b_average, var_ub_average = 0, 0, 0\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    x_samples = ss.poisson.rvs(mu, size=n)\n",
    "    mean_average += np.mean(x_samples)             # sum all the sample means\n",
    "    var_b_average += np.var(x_samples)             # sum all the biased sample variances\n",
    "    var_ub_average += np.var(x_samples, ddof=1)    # sum all the unbiased sample variances\n",
    "\n",
    "mean_average /= num_experiments   # average over all experiments\n",
    "var_b_average /= num_experiments\n",
    "var_ub_average /= num_experiments\n",
    "\n",
    "print(\"The true values of the mean and variance are\", mu, \"and\", mu, \"respectively\")\n",
    "print(\"The average of the sample mean over \" + str(num_experiments) + \" experiments is\", mean_average)\n",
    "print(\"The average of the biased sample variance over \" + str(num_experiments) + \" experiments is\", var_b_average)\n",
    "print(\"The average of the unbiased sample variance over \" + str(num_experiments) + \" experiments is\", var_ub_average)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83b129",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mu, n = 5, 6\n",
    "num_experiments = 10000\n",
    "\n",
    "mean_average, var_b_average, var_ub_average = 0, 0, 0\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    x_samples = ss.poisson.rvs(mu, size=n)\n",
    "    mean_average += np.mean(x_samples)             # sum all the sample means\n",
    "    var_b_average += np.var(x_samples)             # sum all the biased sample variances\n",
    "    var_ub_average += np.var(x_samples, ddof=1)    # sum all the unbiased sample variances\n",
    "\n",
    "mean_average /= num_experiments   # average over all experiments\n",
    "var_b_average /= num_experiments\n",
    "var_ub_average /= num_experiments\n",
    "\n",
    "print(\"The true values of the mean and variance are\", mu, \"and\", mu, \"respectively\")\n",
    "print(\"The average of the sample mean over \" + str(num_experiments) + \" experiments is\", mean_average)\n",
    "print(\"The average of the biased sample variance over \" + str(num_experiments) + \" experiments is\", var_b_average)\n",
    "print(\"The average of the unbiased sample variance over \" + str(num_experiments) + \" experiments is\", var_ub_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f8be7",
   "metadata": {},
   "source": [
    "You should see that the averages of the sample mean and unbiased sample variance closely agree with the true values of the mean and variance of the underlying distribution, while the average of the biased sample variance underestimates the true value of the variance (by a factor of approximately $\\frac{n-1}{n}$).\n",
    "\n",
    "Try playing around with some of the parameters in the program above to see what effect it has. Note that for a large number of samples $n$, the difference between the unbiased and biased sample variance becomes small (since $\\frac{n-1}{n}\\simeq 1$ for large $n$).\n",
    "\n",
    "Repeat this example for a normal distribution with mean 5 and standard deviation 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6ba7f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mu, sigma, n = 5, 3, 6\n",
    "num_experiments = 10000\n",
    "\n",
    "mean_average, var_b_average, var_ub_average = 0, 0, 0\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    x_samples = ss.norm.rvs(loc=mu, scale=sigma, size=n)\n",
    "    mean_average += np.mean(x_samples)             # sum all the sample means\n",
    "    var_b_average += np.var(x_samples)             # sum all the biased sample variances\n",
    "    var_ub_average += np.var(x_samples, ddof=1)    # sum all the unbiased sample variances\n",
    "\n",
    "mean_average /= num_experiments   # average over all experiments\n",
    "var_b_average /= num_experiments\n",
    "var_ub_average /= num_experiments\n",
    "\n",
    "print(\"The true values of the mean and variance are\", mu, \"and\", sigma**2, \"respectively\")\n",
    "print(\"The average of the sample mean over \" + str(num_experiments) + \" experiments is\", mean_average)\n",
    "print(\"The average of the biased sample variance over \" + str(num_experiments) + \" experiments is\", var_b_average)\n",
    "print(\"The average of the unbiased sample variance over \" + str(num_experiments) + \" experiments is\", var_ub_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75d569",
   "metadata": {},
   "source": [
    "# Confidence intervals\n",
    "\n",
    "## Normal distribution with unknown $\\mu$, known $\\sigma$\n",
    "\n",
    "Suppose we sample a random variable $X\\sim N(\\mu,\\sigma^2)$ that is distributed normally with mean $\\mu$ and standard deviation $\\sigma$, and obtain the sample $x_1, x_2,\\ldots, x_n$. In lectures we showed that the (two-sided) confidence interval associated with this sample is \n",
    "$$\n",
    "\\left(\\overline{x} - k\\frac{\\sigma}{\\sqrt{n}},\\ \\overline{x}+k\\frac{\\sigma}{\\sqrt{n}}\\right)\n",
    "$$\n",
    "where $k$ takes on various values depending on the confidence percentage we are considering. Note that the endpoints of the CI are statistics, since we are assuming that $\\sigma$ is **known**.\n",
    "Some commonly considered values are\n",
    "\n",
    "| CI% | $k$ |\n",
    "|:---:|:---:|\n",
    "| 90 | 1.645 |\n",
    "| 95 | 1.96 |\n",
    "| 98 | 2.326 |\n",
    "| 99 | 2.576 |\n",
    "\n",
    "(The general formula for $k$ for a given confidence percentage $C$ is\n",
    "$\n",
    "k = G\\left(\\frac{1}{2}\\left(1+\\frac{C}{100}\\right)\\right)\n",
    "$,\n",
    "where $G$ is the so-called *percent-point function* corresponding to the normal distribution. The percent-point function $G(\\alpha)$ gives the value such that $P(Z<G(\\alpha)) = \\alpha$, where $Z\\sim N(0,1)$. The percent-point function is implemented as `scipy.stats.norm.ppf` in SciPy.)\n",
    "\n",
    "The code below samples a random variable $X\\sim N(5, 3^2)$ 1000 times, and calculates the corresponding 95% confidence interval:\n",
    "\n",
    "```python\n",
    "mu, sigma, n = 5, 3, 1000\n",
    "confidence_percentage = 95\n",
    "k = ss.norm.ppf(1/2*(1+confidence_percentage/100))\n",
    "\n",
    "x_samples = ss.norm.rvs(loc = mu, scale = sigma, size=n)\n",
    "\n",
    "x_bar = np.mean(x_samples)\n",
    "lower_bound, upper_bound = x_bar-k*sigma/np.sqrt(n), x_bar+k*sigma/np.sqrt(n)\n",
    "\n",
    "print(\"The \"+ str(confidence_percentage) + \"% confidence interval for the sample is \"+\"(\" + str(lower_bound) + \",\" + str(upper_bound) + \")\")\n",
    "```\n",
    "Try this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc9016",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mu, sigma, n = 5, 3, 1000\n",
    "confidence_percentage = 95\n",
    "k = ss.norm.ppf(1/2*(1+confidence_percentage/100))\n",
    "\n",
    "x_samples = ss.norm.rvs(loc = mu, scale = sigma, size=n)\n",
    "\n",
    "x_bar = np.mean(x_samples)\n",
    "lower_bound, upper_bound = x_bar-k*sigma/np.sqrt(n), x_bar+k*sigma/np.sqrt(n)\n",
    "\n",
    "print(\"The \"+ str(confidence_percentage) + \"% confidence interval for the sample is \"+\"(\" + str(lower_bound) + \",\" + str(upper_bound) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d287c98",
   "metadata": {},
   "source": [
    "The confidence interval can also be calculated using the `interval()` method, which calculates the interval that contains 95% of the probability of a distribution. Since for the sample mean \n",
    "$$\\overline{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right),$$\n",
    "we calculate the 95% confidence interval using:\n",
    "```python\n",
    "ss.norm.interval(0.95,np.mean(x_samples),sigma/np.sqrt(n))\n",
    "```\n",
    "Try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7bc80",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\"Confidence interval generated using scipy.stats.norm.interval is\", ss.norm.interval(0.95,np.mean(x_samples),sigma/np.sqrt(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a868f9",
   "metadata": {},
   "source": [
    "The interpretation of the 95% confidence interval is as follows: suppose we conduct a number of \"experiments\", where each experiment consists of sampling $n$ times from $X$, and calculating the corresponding confidence interval. Then for a large number of experiments, the true value of the (unknown to us) mean $\\mu$ will lie within the confidence interval 95% of the time. Let's test this with the following code:\n",
    "```python\n",
    "mu, sigma, n = 5, 3, 1000  # n = number of samples taken in each experiment\n",
    "confidence_percentage = 95  # 95% confidence interval\n",
    "k = ss.norm.ppf(1/2*(1+confidence_percentage/100))\n",
    "\n",
    "num_experiments = 100  \n",
    "num_containing_mu = 0        # records the number of times our confidence interval contains the true value of the mean\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    x_samples = ss.norm.rvs(loc=mu, scale=sigma, size=n)\n",
    "    x_bar = np.mean(x_samples)\n",
    "    lower_bound, upper_bound = x_bar-k*sigma/np.sqrt(n), x_bar+k*sigma/np.sqrt(n) # calculating the confidence interval\n",
    "    if (mu >= lower_bound) and (mu <= upper_bound):\n",
    "        num_containing_mu += 1       # if mu lies in the confidence interval, increment num_containing_mu by 1\n",
    "\n",
    "print(\"The fraction of experiments where the confidence interval contains the true value of mu is\", num_containing_mu/num_experiments)\n",
    "```\n",
    "Try this below. Experiment with adjusting `num_experiments`: as it gets larger, the fraction of times the CI contains $\\mu$ should get closer to 95%. Also experiment with other values of `confidence_percentage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cce578",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mu, sigma, n = 5, 3, 1000  # n = number of samples taken in each experiment\n",
    "confidence_percentage = 95  # 95% confidence interval\n",
    "k = ss.norm.ppf(1/2*(1+confidence_percentage/100))\n",
    "\n",
    "num_experiments = 100\n",
    "num_containing_mu = 0        # records the number of times our confidence interval contains the true value of the mean\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    x_samples = ss.norm.rvs(loc=mu, scale=sigma, size=n)\n",
    "    x_bar = np.mean(x_samples)\n",
    "    lower_bound, upper_bound = x_bar-k*sigma/np.sqrt(n), x_bar+k*sigma/np.sqrt(n) # calculating the confidence interval\n",
    "    if (mu >= lower_bound) and (mu <= upper_bound):\n",
    "        num_containing_mu += 1       # if mu lies in the confidence interval, increment num_containing_mu by 1\n",
    "\n",
    "print(\"The fraction of experiments where the confidence interval contains the true value of mu is\", num_containing_mu/num_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a25a1",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimators (MLEs)\n",
    "\n",
    "## Normal distribution with unknown $\\mu$, known $\\sigma$ \n",
    "\n",
    "**scipy.stats** has a built-n `fit()` method that can calculate maximum likelihood estimates for its various distributions. We use it as follows:\n",
    "\n",
    "```python\n",
    "mu_true, sigma_true = 5, 2\n",
    "x_samples = ss.norm.rvs(loc=mu_true, scale=sigma_true, size=1000)  # take 1000 samples\n",
    "\n",
    "mu_MLE = ss.norm.fit(x_samples, fscale=sigma_true)[0]\n",
    "print(\"The maximum likelihood estimate of the mean is\", mu_MLE)\n",
    "\n",
    "```\n",
    "\n",
    "The `.fit()` method returns a list containing the MLE estimates for the given sample. In this example, we imagine we know the standard deviation $\\sigma$, and are just interested in estimating the mean $\\mu$. The optional argument `fscale=sigma_true` tells SciPy to fix the `scale` argument in the distribution (which corresponds to the standard deviation), so that maximises on the `loc` argument only (which corresponds to the mean).\n",
    "\n",
    "Try this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c48f24",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mu_true, sigma_true = 5, 2\n",
    "x_samples = ss.norm.rvs(loc=mu_true, scale=sigma_true, size=1000) # take 1000 samples\n",
    "\n",
    "mu_MLE = ss.norm.fit(x_samples, fscale=sigma_true)[0]\n",
    "print(\"The maximum likelihood estimate of the mean is\", mu_MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa7a67f",
   "metadata": {},
   "source": [
    "We can compare this to the theoretical MLE estimator derived in the lectures (which turned out to just be the sample mean):\n",
    "$$\n",
    "\\mu_\\text{MLE}(x_1,\\ldots,x_n) = \\frac{1}{n}\\sum_{i=1}x_i\n",
    "$$\n",
    "Calculate this value below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf71e36",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(x_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ce5b3",
   "metadata": {},
   "source": [
    "You should see that the two values agree. Let's plot the histogram of the samples we drew, alongside the normal distribution with the given (known) standard deviation, and the MLE estimate for the mean:\n",
    "```python\n",
    "fig, ax = plt.subplots(nrows = 1, ncols=2)\n",
    "\n",
    "ax[0].hist(x_samples, bins=50)\n",
    "ax[0].set_title(\"histogram of samples\")\n",
    "\n",
    "x_range = np.linspace(min(x_samples),max(x_samples),100)\n",
    "ax[1].plot(x_range, ss.norm.pdf(x_range, loc=mu_MLE, scale=sigma_true))\n",
    "ax[1].set_title(\"pdf using mu_MLE\")\n",
    "ax[1].set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "The expression `np.linspace(min, max, num)` returns a list of `num` values starting from `min` and ending with `max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291e75f",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols=2)\n",
    "\n",
    "ax[0].hist(x_samples, bins=50)\n",
    "ax[0].set_title(\"histogram of samples\")\n",
    "\n",
    "x_range = np.linspace(min(x_samples),max(x_samples),100)\n",
    "ax[1].plot(x_range, ss.norm.pdf(x_range, loc=mu_MLE, scale=sigma_true))\n",
    "ax[1].set_title(\"pdf using mu_MLE\")\n",
    "ax[1].set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449837b",
   "metadata": {},
   "source": [
    "Using the `fit()` method works well since we are drawing from a known distribution. But let us suppose we did not know the theoretical expression for the MLE estimator (this might be the case, for example, if we were using our own custom probability distribution to model a problem). We would need to resort to numerically maximizing the likelihood function, or more simply, the log-likelihood. Recall the expression for this:\n",
    "$$\n",
    "\\log L(\\lambda_1,\\ldots,\\lambda_l) = \\sum_{i=1}^n \\log f(x_i;\\lambda_1,\\ldots,\\lambda_l)\n",
    "$$\n",
    "where $x_1,\\ldots, x_n$ is the sample, and $\\lambda_1,\\ldots,\\lambda_l$ are the parameters our distribution function $f$. In the case of the normal distribution\n",
    "$$\n",
    "f(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "Since we are initially assuming $\\sigma$ is known, we just consider the function \n",
    "$$\n",
    "\\log L(\\mu) = \\sum_{i=1}^n \\log f(x_i;\\mu,\\sigma).\n",
    "$$\n",
    "\n",
    "We draw 1000 samples, and plot the log-likelihood using the following code:\n",
    "```python\n",
    "def loglikelihood(mu):\n",
    "    value = 0\n",
    "    for x in x_samples:\n",
    "        value += np.log(ss.norm.pdf(x, loc=mu, scale=sigma_true))\n",
    "    return value\n",
    "\n",
    "mu_range = np.linspace(0,10,100)\n",
    "plt.plot(mu_range, loglikelihood(mu_range))\n",
    "plt.title(\"Plot of log-likelihood function\")\n",
    "plt.xlabel(\"mu\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9c6c3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def loglikelihood(mu):\n",
    "    value = 0\n",
    "    for x in x_samples:\n",
    "        value += np.log(ss.norm.pdf(x, loc=mu, scale=sigma_true))\n",
    "    return value\n",
    "\n",
    "mu_range = np.linspace(0,10,100)\n",
    "plt.plot(mu_range, loglikelihood(mu_range))\n",
    "plt.title(\"Plot of log-likelihood function\")\n",
    "plt.xlabel(\"mu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc8670",
   "metadata": {},
   "source": [
    "Our goal is to find the value of $\\mu$ that maximises the log-likelihood function. We can do this by using the `minimize` function from `scipy.optimize`. Maximizing $L(\\mu)$ is equivalent to minimizing $-L(\\mu)$, so we need to define the auxilliary function `minusloglikelihood`. Also, the minimize function requires an initial guess, which we can estimate from the plot above - we take 7 as our initial guess in the code below:\n",
    "\n",
    "```python\n",
    "import scipy.optimize as so\n",
    "\n",
    "def minusloglikelihood(mu):\n",
    "    return (-1)*loglikelihood(mu)\n",
    "\n",
    "minimization_result = so.minimize(minusloglikelihood, 7)  # minimize with an initial guess of mu=7\n",
    "print(minimization_result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb93fe",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as so\n",
    "\n",
    "def minusloglikelihood(mu):\n",
    "    return (-1)*loglikelihood(mu)\n",
    "\n",
    "minimization_result = so.minimize(minusloglikelihood, 7)  # minimize with an initial guess of mu=7\n",
    "print(minimization_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e80090",
   "metadata": {},
   "source": [
    "`minimization_result` contains a collection of useful information related to the minimization. The value of $\\mu$ that actually minimizes $-\\log L(\\mu)$ is stored in `minimization_result.x` as a numpy array. From this, you can extract the MLE estimate for $\\mu$:\n",
    "```python\n",
    "mu_MLE = minimization_result.x[0]\n",
    "```\n",
    "As above, try plotting the histogram of samples against the pdf generated using $\\mu_\\text{MLE}$ (and the true value of $\\sigma$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9c382",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mu_MLE = minimization_result.x[0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols=2)\n",
    "\n",
    "ax[0].hist(x_samples, bins=50)\n",
    "ax[0].set_title(\"histogram of samples\")\n",
    "\n",
    "x_range = np.linspace(min(x_samples),max(x_samples),100)\n",
    "ax[1].plot(x_range, ss.norm.pdf(x_range, loc=mu_MLE, scale=sigma_true))\n",
    "ax[1].set_title(\"pdf using mu_MLE\")\n",
    "ax[1].set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
